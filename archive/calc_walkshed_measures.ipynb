{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do these need to be rounded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "import os\n",
    "import numpy as np\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features import GeoSeriesAccessor\n",
    "import pandas as pd\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.pivot_table(df, values='a', index='b', columns='c', aggfunc='sum', fill_value=0)\n",
    "# pd.DataFrame.spatial.from_featureclass(???)  \n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)  \n",
    "\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(df['SHAPE'])  \n",
    "# df['AREA'] = gsa.area  # KNOW YOUR UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Outputs'):\n",
    "    os.makedirs('Outputs')\n",
    "    \n",
    "outputs = ['.\\\\Outputs', \"scratch.gdb\", 'results.gdb']\n",
    "gdb = os.path.join(outputs[0], outputs[1])\n",
    "gdb2 = os.path.join(outputs[0], outputs[2])\n",
    "\n",
    "if not arcpy.Exists(gdb):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[1])\n",
    "\n",
    "if not arcpy.Exists(gdb2):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_900 = r'E:\\Projects\\REMM-Manage-Base-Year-Data\\Ancillary\\TAZ.gdb\\TAZ_900_withSmallArea'\n",
    "# city_900 = r'E:\\Projects\\REMM-Manage-Base-Year-Data\\Ancillary\\TAZ.gdb\\CITYAREA_900'\n",
    "parcels = r'E:\\Projects\\REMM-Manage-Base-Year-Data\\Current_Inputs\\remm_base_year.gdb\\parcels'\n",
    "parcel_pts = arcpy.FeatureToPoint_management(parcels, os.path.join(gdb, 'parcel_pts'), \"INSIDE\")\n",
    "parcel_eq = pd.read_csv(r\"E:\\parcel_eq_v7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update parcel eq\n",
    "# small_area = pd.DataFrame.spatial.from_featureclass(r\"E:\\Tasks\\WFRC_Dashboard\\Outputs\\parcel_small_area\")[['parcel_id', 'NewSA']].copy()\n",
    "# small_area.rename({'NewSA':'SMALLAREA'}, axis=1, inplace=True)\n",
    "# parcel_eq = parcel_eq.merge(small_area, on='parcel_id', how='left')\n",
    "# parcel_eq.to_csv(r\"E:\\parcel_eq_v7.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_walkshed_2019 = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\transit_walkshed_2019\"\n",
    "transit_walkshed_2028 = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\transit_walkshed_2028\"\n",
    "transit_walkshed_2032 = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\transit_walkshed_2032\"\n",
    "transit_walkshed_2042 = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\transit_walkshed_2042\"\n",
    "transit_walkshed_2050 = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\transit_walkshed_2050\"\n",
    "parks_walkshed = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\parks_walkshed\"\n",
    "trails_walkshed = r\".\\Inputs\\Walksheds\\Walksheds.gdb\\trails_walkshed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "2020\n",
      "2022\n",
      "2024\n",
      "2026\n",
      "2028\n",
      "2030\n",
      "2032\n",
      "2034\n",
      "2036\n",
      "2038\n",
      "2040\n",
      "2042\n",
      "2044\n",
      "2046\n",
      "2048\n",
      "2050\n"
     ]
    }
   ],
   "source": [
    "# prep the taz table\n",
    "taz_df = pd.DataFrame.spatial.from_featureclass(taz_900)[['SA_TAZID', 'CO_TAZID', 'CO_FIPS', 'CO_NAME', 'SMALLAREA', 'CityArea','DEVACRES', 'SHAPE']].copy()\n",
    "taz_df.rename({'SA_TAZID':'TAZID_900', 'CityArea':'CITYAREA'}, axis=1, inplace=True)\n",
    "transit_taz_df = taz_df.copy()\n",
    "parks_taz_df = taz_df.copy()\n",
    "trailheads_taz_df = taz_df.copy()\n",
    "centers_taz_df = taz_df.copy()\n",
    "\n",
    "# # prep the city area table\n",
    "# city_df = pd.DataFrame.spatial.from_featureclass(city_900)[['CityArea','DEVACRES', 'SHAPE']].copy()\n",
    "# city_df.rename({'CityArea':'CityArea'}, axis=1, inplace=True)\n",
    "# transit_city_df = city_df.copy()\n",
    "# parks_city_df = city_df.copy()\n",
    "# trailheads_city_df = city_df.copy()\n",
    "# centers_city_df = city_df.copy()\n",
    "\n",
    "for year in range(2019, 2051):\n",
    "    if (year % 2 ==0) or (year == 2019): print(year)\n",
    "    short_year = str(year)[2:]\n",
    "    \n",
    "    ###########\n",
    "    # Transit\n",
    "    ###########\n",
    "\n",
    "    # use the year to select the walkshed\n",
    "    if (year < 2028): transit_walkshed = transit_walkshed_2019\n",
    "    if (year >= 2028) and (year < 2032): transit_walkshed = transit_walkshed_2028\n",
    "    if (year >= 2032) and (year < 2042): transit_walkshed = transit_walkshed_2032\n",
    "    if (year >= 2042) and (year < 2050): transit_walkshed = transit_walkshed_2042\n",
    "    if (year >= 2050): transit_walkshed = transit_walkshed_2050\n",
    "\n",
    "    # create a layer and filter for the 20 minute polygon \n",
    "    transit_walkshed_lyr =  arcpy.MakeFeatureLayer_management(transit_walkshed, 'transit_walkshed_lyr', where_clause=\"\"\" ToBreak = 20 \"\"\") \n",
    "\n",
    "    # spatial join to walkshed areas\n",
    "    target_features = parcel_pts \n",
    "    join_features = transit_walkshed_lyr\n",
    "    output_features = os.path.join(gdb, \"pts_transit_walkshed_sj\")\n",
    "\n",
    "    fieldmappings = arcpy.FieldMappings()\n",
    "    fieldmappings.addTable(target_features)\n",
    "    fieldmappings.addTable(join_features)\n",
    "\n",
    "    transit_sj = arcpy.SpatialJoin_analysis(target_features, join_features, output_features,'JOIN_ONE_TO_ONE', \"KEEP_ALL\", fieldmappings, match_option=\"INTERSECT\")\n",
    "\n",
    "    transit_sj_df = pd.DataFrame.spatial.from_featureclass(transit_sj[0])[['parcel_id','ToBreak']].copy()\n",
    "    transit_sj_df.loc[transit_sj_df['ToBreak'] == 20, 'transit'] = 1\n",
    "    transit_sj_df = transit_sj_df[['parcel_id','transit']].copy()\n",
    "\n",
    "    ###########\n",
    "    # Parks\n",
    "    ###########\n",
    "    \n",
    "    # create a layer and filter for the 20 minute polygon \n",
    "    parks_walkshed_lyr =  arcpy.MakeFeatureLayer_management(parks_walkshed, 'parks_walkshed_lyr', where_clause=\"\"\" ToBreak = 20 \"\"\") \n",
    "\n",
    "    # spatial join to walkshed areas\n",
    "    target_features = parcel_pts \n",
    "    join_features = parks_walkshed_lyr\n",
    "    output_features = os.path.join(gdb, \"pts_parks_walkshed_sj\")\n",
    "\n",
    "    fieldmappings = arcpy.FieldMappings()\n",
    "    fieldmappings.addTable(target_features)\n",
    "    fieldmappings.addTable(join_features)\n",
    "\n",
    "    parks_sj = arcpy.SpatialJoin_analysis(target_features, join_features, output_features,'JOIN_ONE_TO_ONE', \"KEEP_ALL\", fieldmappings, match_option=\"INTERSECT\")\n",
    "\n",
    "    parks_sj_df = pd.DataFrame.spatial.from_featureclass(parks_sj[0])[['parcel_id','ToBreak']].copy()\n",
    "    parks_sj_df.loc[parks_sj_df['ToBreak'] == 20, 'parks'] = 1\n",
    "    parks_sj_df = parks_sj_df[['parcel_id','parks']].copy()\n",
    "\n",
    "    #############\n",
    "    # Trailheads\n",
    "    #############\n",
    "\n",
    "    # create a layer and filter for the 20 minute polygon \n",
    "    trails_walkshed_lyr =  arcpy.MakeFeatureLayer_management(trails_walkshed, 'trails_walkshed_lyr', where_clause=\"\"\" ToBreak = 20 \"\"\") \n",
    "\n",
    "    # spatial join to walkshed areas\n",
    "    target_features = parcel_pts \n",
    "    join_features = trails_walkshed_lyr\n",
    "    output_features = os.path.join(gdb, \"pts_trailheads_walkshed_sj\")\n",
    "\n",
    "    fieldmappings = arcpy.FieldMappings()\n",
    "    fieldmappings.addTable(target_features)\n",
    "    fieldmappings.addTable(join_features)\n",
    "\n",
    "    trails_sj = arcpy.SpatialJoin_analysis(target_features, join_features, output_features,'JOIN_ONE_TO_ONE', \"KEEP_ALL\", fieldmappings, match_option=\"INTERSECT\")\n",
    "\n",
    "    trails_sj_df = pd.DataFrame.spatial.from_featureclass(trails_sj[0])[['parcel_id','ToBreak']].copy()\n",
    "    trails_sj_df.loc[trails_sj_df['ToBreak'] == 20, 'trailheads'] = 1\n",
    "    trails_sj_df = trails_sj_df[['parcel_id','trailheads']].copy()\n",
    "\n",
    "    ##################\n",
    "    # merge data\n",
    "    ##################\n",
    "\n",
    "    # read in se data\n",
    "    parsel_se = pd.read_pickle(r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\Parcel_SE_6Runs_Average_20230613\\averaged_parcel_se_{}.pkl\".format(year))\n",
    "\n",
    "    # parcel_eq.rename({\"CITY_AREA\": \"CityArea\"}, axis=1, inplace=True)\n",
    "    parsel_se = parsel_se.merge(parcel_eq, on='parcel_id', how='left')\n",
    "    \n",
    "    parsel_se = parsel_se.merge(transit_sj_df, on ='parcel_id', how='left')\n",
    "    parsel_se = parsel_se.merge(parks_sj_df, on ='parcel_id', how='left')\n",
    "    parsel_se = parsel_se.merge(trails_sj_df, on ='parcel_id', how='left')\n",
    "\n",
    "    # transit\n",
    "    parcel_se_transit = parsel_se[parsel_se['transit']==1].copy()\n",
    "    taz_transit = parcel_se_transit.groupby('TAZID_900', as_index=False)[['households']].sum()\n",
    "    taz_transit.columns = ['TAZID_900', f'HH_{year}']\n",
    "    # city_transit = parcel_se_transit.groupby('CityArea', as_index=False)[['households']].sum()\n",
    "    # city_transit.columns = ['CityArea', f'YEAR{short_year}']\n",
    "\n",
    "    # parks\n",
    "    parcel_se_parks = parsel_se[parsel_se['parks']==1].copy()\n",
    "    taz_parks = parcel_se_parks.groupby('TAZID_900', as_index=False)[['households']].sum()\n",
    "    taz_parks.columns = ['TAZID_900', f'HH_{year}']\n",
    "    # city_parks = parcel_se_parks.groupby('CityArea', as_index=False)[['households']].sum()\n",
    "    # city_parks.columns = ['CityArea', f'YEAR{short_year}']\n",
    "\n",
    "    # trailheads\n",
    "    parcel_se_trailheads = parsel_se[parsel_se['trailheads']==1].copy()\n",
    "    taz_trailheads = parcel_se_trailheads.groupby('TAZID_900', as_index=False)[['households']].sum()\n",
    "    taz_trailheads.columns = ['TAZID_900', f'HH_{year}']\n",
    "    # city_trailheads = parcel_se_trailheads.groupby('CityArea', as_index=False)[['households']].sum()\n",
    "    # city_trailheads.columns = ['CityArea', f'YEAR{short_year}']\n",
    "\n",
    "    # centers\n",
    "    parcel_se_centers = parsel_se[parsel_se['CENTER_NAME']!= \"Non-center\"].copy()\n",
    "    taz_centers = parcel_se_centers.groupby('TAZID_900', as_index=False)[['hhpop']].sum()\n",
    "    taz_centers.columns = ['TAZID_900', f'POP_{year}']\n",
    "    # city_centers = parcel_se_centers.groupby('CityArea', as_index=False)[['hhpop']].sum()\n",
    "    # city_centers.columns = ['CityArea', f'YEAR{short_year}']\n",
    "\n",
    "    # merge back to taz geometry\n",
    "    transit_taz_df = transit_taz_df.merge(taz_transit, on='TAZID_900', how='left')\n",
    "    parks_taz_df = parks_taz_df.merge(taz_parks, on='TAZID_900', how='left')\n",
    "    trailheads_taz_df = trailheads_taz_df.merge(taz_trailheads, on='TAZID_900', how='left')\n",
    "    centers_taz_df = centers_taz_df.merge(taz_centers, on='TAZID_900', how='left')\n",
    "\n",
    "    # # merge back to city area geometry\n",
    "    # transit_city_df = transit_city_df.merge(city_transit, on='CityArea', how='left')\n",
    "    # parks_city_df = parks_city_df.merge(city_parks, on='CityArea', how='left')\n",
    "    # trailheads_city_df = trailheads_city_df.merge(city_trailheads, on='CityArea', how='left')\n",
    "    # centers_city_df = centers_city_df.merge(city_centers, on='CityArea', how='left')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Tasks\\\\WFRC_Dashboard\\\\Outputs\\\\results.gdb\\\\population_within_centers_by_taz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transit_taz_df = fill_na_sedf(transit_taz_df)\n",
    "parks_taz_df = fill_na_sedf(parks_taz_df)\n",
    "trailheads_taz_df = fill_na_sedf(trailheads_taz_df)\n",
    "centers_taz_df = fill_na_sedf(centers_taz_df)\n",
    "\n",
    "transit_taz_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_transit_by_taz'),sanitize_columns=False)\n",
    "parks_taz_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_parks_by_taz'),sanitize_columns=False)\n",
    "trailheads_taz_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_trailheads_by_taz'),sanitize_columns=False)\n",
    "centers_taz_df.spatial.to_featureclass(location=os.path.join(gdb2,'population_within_centers_by_taz'),sanitize_columns=False)\n",
    "\n",
    "# transit_city_df = fill_na_sedf(transit_city_df)\n",
    "# parks_city_df = fill_na_sedf(parks_city_df)\n",
    "# trailheads_city_df = fill_na_sedf(trailheads_city_df)\n",
    "# centers_city_df = fill_na_sedf(centers_city_df)\n",
    "\n",
    "# transit_city_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_transit_by_city'),sanitize_columns=False)\n",
    "# parks_city_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_parks_by_city'),sanitize_columns=False)\n",
    "# trailheads_city_df.spatial.to_featureclass(location=os.path.join(gdb2,'households_within_20min_walk_to_trailheads_by_city'),sanitize_columns=False)\n",
    "# centers_city_df.spatial.to_featureclass(location=os.path.join(gdb2,'population_within_centers_by_city'),sanitize_columns=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
